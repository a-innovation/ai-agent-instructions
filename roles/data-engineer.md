# DATA ENGINEER AI - AGENT INSTRUCTIONS v3.1

> Extends: EXPERT_DEVELOPER_AI_CORE_INSTRUCTIONS. md
> Specialization: Data Pipelines, ETL/ELT, Data Warehousing, Analytics Engineering

---

## ðŸŽ­ DATA ENGINEERING THINKING MODES

| Mode | Data Focus |
|------|------------|
| **ðŸ—ï¸ Architect** | Data modeling, pipeline design, scalability, schema evolution, partitioning strategies |
| **ðŸ›¡ï¸ Sentry** | Data quality, PII protection, access control, compliance (GDPR/CCPA), data lineage |
| **ðŸ¤– Automator** | Pipeline orchestration, testing, monitoring, self-healing jobs, alerting |

---

## ðŸŽ¯ DATA ENGINEERING PRINCIPLES

| Principle | Description |
|-----------|-------------|
| **Idempotent Pipelines** | Re-running produces same result, safe to retry |
| **Schema-First** | Define contracts before implementation |
| **Incremental Processing** | Process only new/changed data when possible |
| **Quality as Code** | Automated data validation and testing |
| **Lineage & Observability** | Track data from source to consumption |
| **Immutable Raw Data** | Never modify source data, transform into new layers |
| **Reproducibility** | Any output can be recreated from inputs |
| **Fail Fast, Recover Gracefully** | Detect issues early, enable easy recovery |

---

## ðŸ“ PROJECT STRUCTURE

### Modern Data Stack Project

```
data-platform/
â”œâ”€â”€ dbt/                           # Analytics Engineering
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # 1:1 with source tables
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_customers.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/         # Business logic transformations
â”‚   â”‚   â”‚   â””â”€â”€ int_orders_enriched.sql
â”‚   â”‚   â”œâ”€â”€ marts/                # Business-ready tables
â”‚   â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customers.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ fct_orders.sql
â”‚   â”‚   â”‚   â””â”€â”€ marketing/
â”‚   â”‚   â”‚       â””â”€â”€ mrt_campaign_performance.sql
â”‚   â”‚   â””â”€â”€ utilities/            # Helper models
â”‚   â”‚       â””â”€â”€ date_spine.sql
â”‚   â”œâ”€â”€ macros/                   # Reusable SQL functions
â”‚   â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â”‚   â””â”€â”€ cents_to_dollars. sql
â”‚   â”œâ”€â”€ tests/                    # Custom data tests
â”‚   â”‚   â””â”€â”€ assert_positive_amounts.sql
â”‚   â”œâ”€â”€ seeds/                    # Static reference data
â”‚   â”‚   â””â”€â”€ country_codes.csv
â”‚   â”œâ”€â”€ snapshots/                # SCD Type 2 tracking
â”‚   â”‚   â””â”€â”€ customer_snapshot.sql
â”‚   â”œâ”€â”€ analyses/                 # Ad-hoc queries
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ airflow/                       # Orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ data_ingestion_dag.py
â”‚   â”‚   â”œâ”€â”€ dbt_transformation_dag.py
â”‚   â”‚   â””â”€â”€ data_quality_dag.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â””â”€â”€ custom_operators/
â”‚   â””â”€â”€ config/
â”‚
â”œâ”€â”€ spark/                         # Big Data Processing
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ aggregation/
â”‚   â”œâ”€â”€ lib/                      # Shared utilities
â”‚   â”‚   â”œâ”€â”€ schema_registry.py
â”‚   â”‚   â””â”€â”€ quality_checks.py
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ ingestion/                     # Data Ingestion
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ salesforce/
â”‚   â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”œâ”€â”€ schemas/                  # Source schemas
â”‚   â”‚   â””â”€â”€ salesforce_contacts.json
â”‚   â””â”€â”€ config/
â”‚
â”œâ”€â”€ infrastructure/                # IaC for Data Platform
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ bigquery/
â”‚   â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ storage/
â”‚   â””â”€â”€ kubernetes/
â”‚
â”œâ”€â”€ data_quality/                  # Data Quality Framework
â”‚   â”œâ”€â”€ great_expectations/
â”‚   â”‚   â”œâ”€â”€ expectations/
â”‚   â”‚   â””â”€â”€ checkpoints/
â”‚   â””â”€â”€ soda/
â”‚       â””â”€â”€ checks/
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ data_dictionary/
â”‚   â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ runbooks/
â”‚
â”œâ”€â”€ tests/                         # Integration tests
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â””â”€â”€ scripts/                       # Utility scripts
    â”œâ”€â”€ backfill. py
    â””â”€â”€ data_validation.py
```

### Python Data Pipeline Project

```
data-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_pipeline.py
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ source_extractor.py
â”‚   â”‚   â”‚   â””â”€â”€ incremental_loader.py
â”‚   â”‚   â””â”€â”€ transformation/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ customer_transformer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database. py
â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â””â”€â”€ cloud_storage.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ source_models.py
â”‚   â”‚   â””â”€â”€ target_models.py
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ validators.py
â”‚   â”‚   â””â”€â”€ expectations.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ retry. py
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__. py
â”‚       â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ðŸ—ï¸ DATA MODELING PATTERNS

### Dimensional Modeling (Kimball)

```markdown
## Star Schema Design

### Fact Tables
- Grain: One row per [business event]
- Contains: Measurements, metrics, foreign keys
- Types: Transaction, Periodic Snapshot, Accumulating Snapshot

### Dimension Tables
- Contains: Descriptive attributes
- Types: Conformed, Role-Playing, Junk, Degenerate

### Design Checklist
- [ ] Business process identified
- [ ] Grain declared
- [ ] Dimensions identified
- [ ] Facts identified
- [ ] Slowly changing dimensions strategy defined
```

```sql
-- Example: Sales Star Schema

-- Fact Table
CREATE TABLE fct_sales (
    sale_key BIGINT GENERATED ALWAYS AS IDENTITY,
    
    -- Dimension Keys (Foreign Keys)
    date_key INT NOT NULL,
    customer_key INT NOT NULL,
    product_key INT NOT NULL,
    store_key INT NOT NULL,
    
    -- Degenerate Dimension
    order_number VARCHAR(50) NOT NULL,
    
    -- Measures
    quantity INT NOT NULL,
    unit_price_cents BIGINT NOT NULL,
    discount_cents BIGINT DEFAULT 0,
    total_amount_cents BIGINT NOT NULL,
    
    -- Metadata
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    _source_system VARCHAR(50) NOT NULL,
    
    PRIMARY KEY (sale_key)
)
PARTITION BY RANGE (date_key);

-- Dimension Table with SCD Type 2
CREATE TABLE dim_customer (
    customer_key BIGINT GENERATED ALWAYS AS IDENTITY,
    
    -- Natural Key
    customer_id VARCHAR(50) NOT NULL,
    
    -- Attributes
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    email VARCHAR(255),
    customer_segment VARCHAR(50),
    lifetime_value_tier VARCHAR(20),
    
    -- SCD Type 2 Tracking
    effective_from DATE NOT NULL,
    effective_to DATE DEFAULT '9999-12-31',
    is_current BOOLEAN DEFAULT TRUE,
    
    -- Metadata
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    _source_hash VARCHAR(64) NOT NULL,
    
    PRIMARY KEY (customer_key)
);

-- Date Dimension (Role-Playing)
CREATE TABLE dim_date (
    date_key INT PRIMARY KEY,  -- YYYYMMDD format
    full_date DATE NOT NULL,
    
    -- Calendar Attributes
    day_of_week INT,
    day_name VARCHAR(10),
    day_of_month INT,
    day_of_year INT,
    week_of_year INT,
    month_number INT,
    month_name VARCHAR(10),
    quarter INT,
    year INT,
    
    -- Fiscal Calendar
    fiscal_quarter INT,
    fiscal_year INT,
    
    -- Flags
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name VARCHAR(50)
);
```

### Data Vault Modeling

```sql
-- Hub: Business Keys
CREATE TABLE hub_customer (
    hub_customer_hk BYTEA PRIMARY KEY,  -- Hash key
    customer_id VARCHAR(50) NOT NULL,
    load_date TIMESTAMP NOT NULL,
    record_source VARCHAR(100) NOT NULL
);

-- Satellite: Descriptive Attributes
CREATE TABLE sat_customer_details (
    hub_customer_hk BYTEA NOT NULL,
    load_date TIMESTAMP NOT NULL,
    load_end_date TIMESTAMP DEFAULT '9999-12-31',
    
    -- Attributes
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    email VARCHAR(255),
    
    -- Metadata
    hash_diff BYTEA NOT NULL,  -- Hash of attributes for change detection
    record_source VARCHAR(100) NOT NULL,
    
    PRIMARY KEY (hub_customer_hk, load_date)
);

-- Link: Relationships
CREATE TABLE link_customer_order (
    link_customer_order_hk BYTEA PRIMARY KEY,
    hub_customer_hk BYTEA NOT NULL,
    hub_order_hk BYTEA NOT NULL,
    load_date TIMESTAMP NOT NULL,
    record_source VARCHAR(100) NOT NULL
);
```

---

## ðŸ”„ ETL/ELT PATTERNS

### Incremental Loading

```python
# pipelines/incremental_loader.py
from datetime import datetime, timedelta
from typing import Optional
import pandas as pd
from sqlalchemy import text

class IncrementalLoader:
    """
    Handles incremental data loading with watermark tracking. 
    
    Supports:
    - Timestamp-based incremental
    - ID-based incremental
    - CDC (Change Data Capture)
    """
    
    def __init__(
        self,
        source_connection,
        target_connection,
        watermark_table: str = "etl_watermarks"
    ):
        self.source = source_connection
        self.target = target_connection
        self.watermark_table = watermark_table
    
    def get_watermark(self, pipeline_name: str) -> Optional[datetime]:
        """Retrieve last successful watermark."""
        query = text("""
            SELECT watermark_value 
            FROM :watermark_table 
            WHERE pipeline_name = :pipeline_name
            ORDER BY created_at DESC 
            LIMIT 1
        """)
        result = self.target.execute(
            query, 
            {"watermark_table": self.watermark_table, "pipeline_name": pipeline_name}
        ).fetchone()
        return result[0] if result else None
    
    def extract_incremental(
        self,
        table_name: str,
        timestamp_column: str,
        pipeline_name: str,
        lookback_buffer: timedelta = timedelta(hours=1)
    ) -> pd.DataFrame:
        """
        Extract data incrementally based on timestamp.
        
        Args:
            table_name: Source table name
            timestamp_column: Column to use for incremental extraction
            pipeline_name: Unique identifier for watermark tracking
            lookback_buffer: Safety buffer for late-arriving data
        """
        watermark = self.get_watermark(pipeline_name)
        
        if watermark:
            # Apply lookback buffer for late-arriving data
            safe_watermark = watermark - lookback_buffer
            query = f"""
                SELECT * FROM {table_name}
                WHERE {timestamp_column} > :watermark
                ORDER BY {timestamp_column}
            """
            df = pd.read_sql(query, self.source, params={"watermark": safe_watermark})
        else:
            # Initial full load
            query = f"SELECT * FROM {table_name} ORDER BY {timestamp_column}"
            df = pd.read_sql(query, self.source)
        
        return df
    
    def upsert_data(
        self,
        df: pd.DataFrame,
        target_table: str,
        key_columns: list[str],
        update_columns: list[str]
    ) -> int:
        """
        Upsert data using MERGE/ON CONFLICT pattern.
        
        Returns number of rows affected. 
        """
        if df.empty:
            return 0
        
        # Generate UPSERT SQL
        columns = df.columns.tolist()
        key_clause = " AND ".join([f"target.{k} = source.{k}" for k in key_columns])
        update_clause = ", ".join([f"{c} = source.{c}" for c in update_columns])
        insert_columns = ", ".join(columns)
        insert_values = ", ". join([f"source.{c}" for c in columns])
        
        merge_sql = f"""
            MERGE INTO {target_table} AS target
            USING temp_source AS source
            ON {key_clause}
            WHEN MATCHED THEN
                UPDATE SET {update_clause}, _updated_at = CURRENT_TIMESTAMP
            WHEN NOT MATCHED THEN
                INSERT ({insert_columns}, _loaded_at)
                VALUES ({insert_values}, CURRENT_TIMESTAMP)
        """
        
        # Load to temp table and merge
        df.to_sql("temp_source", self.target, if_exists="replace", index=False)
        result = self.target. execute(text(merge_sql))
        
        return result.rowcount
    
    def update_watermark(
        self,
        pipeline_name: str,
        watermark_value: datetime,
        rows_processed: int
    ) -> None:
        """Record successful watermark."""
        insert_sql = text("""
            INSERT INTO :watermark_table 
            (pipeline_name, watermark_value, rows_processed, created_at)
            VALUES (:pipeline_name, :watermark_value, :rows_processed, CURRENT_TIMESTAMP)
        """)
        self.target.execute(insert_sql, {
            "watermark_table": self.watermark_table,
            "pipeline_name": pipeline_name,
            "watermark_value": watermark_value,
            "rows_processed": rows_processed
        })
```

### Idempotent Pipeline Pattern

```python
# pipelines/base_pipeline.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Optional
import hashlib
import logging

logger = logging.getLogger(__name__)

@dataclass
class PipelineContext:
    """Execution context for pipeline runs."""
    run_id: str
    execution_date: datetime
    is_backfill: bool = False
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None

@dataclass
class PipelineResult:
    """Result of pipeline execution."""
    success: bool
    rows_processed: int
    rows_inserted: int
    rows_updated: int
    rows_failed: int
    duration_seconds: float
    error_message: Optional[str] = None

class BasePipeline(ABC):
    """
    Base class for idempotent data pipelines.
    
    Ensures:
    - Re-running produces same result
    - Partial failures can be recovered
    - Full audit trail
    """
    
    def __init__(self, name: str, target_table: str):
        self.name = name
        self.target_table = target_table
        self. logger = logging.getLogger(f"pipeline. {name}")
    
    @abstractmethod
    def extract(self, context: PipelineContext) -> Any:
        """Extract data from source."""
        pass
    
    @abstractmethod
    def transform(self, data: Any, context: PipelineContext) -> Any:
        """Transform extracted data."""
        pass
    
    @abstractmethod
    def load(self, data: Any, context: PipelineContext) -> int:
        """Load data to target."""
        pass
    
    def validate(self, data: Any, context: PipelineContext) -> bool:
        """Validate data before loading.  Override for custom validation."""
        return True
    
    def pre_execute(self, context: PipelineContext) -> None:
        """Hook for pre-execution logic."""
        pass
    
    def post_execute(self, context: PipelineContext, result: PipelineResult) -> None:
        """Hook for post-execution logic."""
        pass
    
    def run(self, context: PipelineContext) -> PipelineResult:
        """
        Execute the pipeline with full error handling and logging.
        
        This method is idempotent - safe to retry on failure.
        """
        start_time = datetime. now()
        
        self.logger.info(f"Starting pipeline run: {context.run_id}")
        
        try:
            # Pre-execution hook
            self.pre_execute(context)
            
            # Extract
            self.logger.info("Extracting data...")
            raw_data = self. extract(context)
            
            # Transform
            self.logger.info("Transforming data...")
            transformed_data = self.transform(raw_data, context)
            
            # Validate
            self. logger.info("Validating data...")
            if not self. validate(transformed_data, context):
                raise ValueError("Data validation failed")
            
            # Load (idempotent - uses MERGE/UPSERT)
            self.logger. info("Loading data...")
            rows_affected = self.load(transformed_data, context)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            result = PipelineResult(
                success=True,
                rows_processed=len(transformed_data) if hasattr(transformed_data, '__len__') else 0,
                rows_inserted=rows_affected,  # Simplified; real implementation would track separately
                rows_updated=0,
                rows_failed=0,
                duration_seconds=duration
            )
            
            # Post-execution hook
            self.post_execute(context, result)
            
            self.logger.info(f"Pipeline completed successfully in {duration:.2f}s")
            return result
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            self.logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
            
            return PipelineResult(
                success=False,
                rows_processed=0,
                rows_inserted=0,
                rows_updated=0,
                rows_failed=0,
                duration_seconds=duration,
                error_message=str(e)
            )
    
    @staticmethod
    def generate_surrogate_key(*values) -> str:
        """Generate deterministic surrogate key from business keys."""
        combined = "|".join(str(v) for v in values)
        return hashlib.sha256(combined.encode()).hexdigest()
```

---

## ðŸ“Š DBT BEST PRACTICES

### Model Organization

```yaml
# dbt_project.yml
name: 'analytics'
version: '1.0. 0'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"

models:
  analytics:
    # Staging: 1:1 with source, light transformations
    staging:
      +materialized: view
      +schema: staging
      +tags: ['staging']
    
    # Intermediate: Business logic, joins
    intermediate:
      +materialized: ephemeral
      +tags: ['intermediate']
    
    # Marts: Business-ready, wide tables
    marts:
      +materialized: table
      +tags: ['marts']
      core:
        +schema: core
      marketing:
        +schema: marketing
      finance:
        +schema: finance

vars:
  # Date range for incremental models
  start_date: '2020-01-01'
  # Lookback for late-arriving data
  lookback_days: 3
```

### Staging Model Template

```sql
-- models/staging/stg_orders.sql

{{
    config(
        materialized='view',
        tags=['staging', 'orders']
    )
}}

with source as (
    select * from {{ source('ecommerce', 'orders') }}
),

renamed as (
    select
        -- Primary Key
        order_id,
        
        -- Foreign Keys
        customer_id,
        
        -- Timestamps (standardized to UTC)
        created_at as ordered_at,
        updated_at,
        shipped_at,
        delivered_at,
        
        -- Status
        lower(trim(status)) as order_status,
        
        -- Amounts (convert to cents for precision)
        cast(subtotal * 100 as integer) as subtotal_cents,
        cast(tax * 100 as integer) as tax_cents,
        cast(shipping * 100 as integer) as shipping_cents,
        cast(total * 100 as integer) as total_cents,
        cast(discount * 100 as integer) as discount_cents,
        
        -- Metadata
        '{{ var("source_system", "ecommerce_db") }}' as _source_system,
        current_timestamp as _loaded_at
        
    from source
    
    -- Exclude test/invalid orders
    where order_id is not null
      and customer_id is not null
      and total > 0
)

select * from renamed
```

### Intermediate Model (Business Logic)

```sql
-- models/intermediate/int_orders_enriched.sql

{{
    config(
        materialized='ephemeral'
    )
}}

with orders as (
    select * from {{ ref('stg_orders') }}
),

customers as (
    select * from {{ ref('stg_customers') }}
),

order_items as (
    select
        order_id,
        count(*) as item_count,
        sum(quantity) as total_quantity
    from {{ ref('stg_order_items') }}
    group by 1
),

enriched as (
    select
        o.*,
        
        -- Customer attributes
        c.customer_segment,
        c. first_order_date,
        c.lifetime_order_count,
        
        -- Order attributes
        oi.item_count,
        oi.total_quantity,
        
        -- Derived metrics
        case
            when o.total_cents >= 10000 then 'high'
            when o. total_cents >= 5000 then 'medium'
            else 'low'
        end as order_value_tier,
        
        -- Time-based attributes
        {{ dbt_utils.date_trunc('day', 'o.ordered_at') }} as order_date,
        {{ dbt_utils.date_trunc('month', 'o. ordered_at') }} as order_month,
        extract(dow from o.ordered_at) as day_of_week,
        
        -- Flags
        case when o.discount_cents > 0 then true else false end as has_discount,
        case when o.shipped_at is not null then true else false end as is_shipped,
        case when o.delivered_at is not null then true else false end as is_delivered
        
    from orders o
    left join customers c on o.customer_id = c. customer_id
    left join order_items oi on o. order_id = oi.order_id
)

select * from enriched
```

### Fact Table (Incremental)

```sql
-- models/marts/core/fct_orders.sql

{{
    config(
        materialized='incremental',
        unique_key='order_key',
        incremental_strategy='merge',
        partition_by={
            'field': 'order_date',
            'data_type': 'date',
            'granularity': 'month'
        },
        cluster_by=['customer_key', 'order_status'],
        tags=['core', 'fact']
    )
}}

with orders as (
    select * from {{ ref('int_orders_enriched') }}
    
    {% if is_incremental() %}
    -- Only process new/updated records
    where updated_at > (
        select coalesce(max(updated_at), '1900-01-01')
        from {{ this }}
    )
    {% endif %}
),

dim_customers as (
    select customer_key, customer_id
    from {{ ref('dim_customers') }}
    where is_current = true
),

dim_dates as (
    select date_key, full_date
    from {{ ref('dim_date') }}
),

final as (
    select
        -- Surrogate Key
        {{ dbt_utils. generate_surrogate_key(['o.order_id']) }} as order_key,
        
        -- Dimension Keys
        dc.customer_key,
        dd.date_key as order_date_key,
        
        -- Degenerate Dimension
        o.order_id,
        
        -- Measures
        o.item_count,
        o.total_quantity,
        o. subtotal_cents,
        o.tax_cents,
        o.shipping_cents,
        o.discount_cents,
        o.total_cents,
        
        -- Attributes
        o.order_status,
        o.order_value_tier,
        o.has_discount,
        o.is_shipped,
        o. is_delivered,
        
        -- Timestamps
        o.ordered_at,
        o.shipped_at,
        o. delivered_at,
        o.updated_at,
        
        -- Metadata
        o._source_system,
        current_timestamp as _loaded_at
        
    from orders o
    left join dim_customers dc on o.customer_id = dc.customer_id
    left join dim_dates dd on o.order_date = dd.full_date
)

select * from final
```

### Dimension with SCD Type 2

```sql
-- snapshots/customer_snapshot.sql

{% snapshot customer_snapshot %}

{{
    config(
        target_schema='snapshots',
        unique_key='customer_id',
        strategy='check',
        check_cols=['first_name', 'last_name', 'email', 'customer_segment'],
        invalidate_hard_deletes=True
    )
}}

select
    customer_id,
    first_name,
    last_name,
    email,
    customer_segment,
    created_at,
    updated_at
from {{ source('ecommerce', 'customers') }}

{% endsnapshot %}

-- models/marts/core/dim_customers. sql

{{
    config(
        materialized='table',
        tags=['core', 'dimension']
    )
}}

with snapshot as (
    select * from {{ ref('customer_snapshot') }}
),

final as (
    select
        -- Surrogate Key
        {{ dbt_utils. generate_surrogate_key(['customer_id', 'dbt_valid_from']) }} as customer_key,
        
        -- Natural Key
        customer_id,
        
        -- Attributes
        first_name,
        last_name,
        email,
        customer_segment,
        
        -- SCD Type 2 Fields
        dbt_valid_from as effective_from,
        coalesce(dbt_valid_to, '9999-12-31'::date) as effective_to,
        case when dbt_valid_to is null then true else false end as is_current,
        
        -- Metadata
        {{ dbt_utils. generate_surrogate_key(['first_name', 'last_name', 'email', 'customer_segment']) }} as _row_hash,
        current_timestamp as _loaded_at
        
    from snapshot
)

select * from final
```

### Custom Tests

```sql
-- tests/assert_referential_integrity.sql

-- Ensure all orders have valid customers
select
    o.order_id,
    o.customer_key
from {{ ref('fct_orders') }} o
left join {{ ref('dim_customers') }} c 
    on o. customer_key = c.customer_key
where c.customer_key is null

-- tests/assert_no_negative_amounts.sql

select
    order_id,
    total_cents
from {{ ref('fct_orders') }}
where total_cents < 0
```

```yaml
# models/marts/core/schema.yml

version: 2

models:
  - name: fct_orders
    description: "Order fact table at the grain of one row per order"
    columns:
      - name: order_key
        description: "Surrogate key for the order"
        tests:
          - unique
          - not_null
      
      - name: customer_key
        description: "Foreign key to dim_customers"
        tests:
          - not_null
          - relationships:
              to: ref('dim_customers')
              field: customer_key
      
      - name: total_cents
        description: "Total order amount in cents"
        tests:
          - not_null
          - dbt_expectations. expect_column_values_to_be_between:
              min_value: 0
              max_value: 100000000  # $1M max
      
      - name: order_status
        tests:
          - accepted_values:
              values: ['pending', 'processing', 'shipped', 'delivered', 'cancelled', 'refunded']

  - name: dim_customers
    description: "Customer dimension with SCD Type 2"
    columns:
      - name: customer_key
        tests:
          - unique
          - not_null
      
      - name: email
        tests:
          - dbt_expectations.expect_column_values_to_match_regex:
              regex: '^[a-zA-Z0-9_. +-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-. ]+$'
```

---

## ðŸ”„ ORCHESTRATION (AIRFLOW)

### DAG Template

```python
# dags/daily_etl_dag.py

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators. bash import BashOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.utils.task_group import TaskGroup
from airflow. models import Variable

# Default arguments
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email': ['data-alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
}

# DAG definition
with DAG(
    dag_id='daily_etl_pipeline',
    default_args=default_args,
    description='Daily ETL pipeline for analytics data',
    schedule_interval='0 6 * * *',  # 6 AM UTC daily
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=['production', 'etl', 'daily'],
    doc_md="""
    # Daily ETL Pipeline
    
    Extracts data from source systems, transforms via dbt, and runs quality checks.
    
    ## Dependencies
    - Source database availability
    - dbt cloud/core
    - Great Expectations
    
    ## Contacts
    - Team: #data-engineering
    - Oncall: data-oncall@company.com
    """,
) as dag:
    
    # =========================================
    # EXTRACTION
    # =========================================
    with TaskGroup(group_id='extraction') as extraction_group:
        
        extract_orders = PythonOperator(
            task_id='extract_orders',
            python_callable=extract_orders_incremental,
            op_kwargs={
                'source_conn_id': 'source_postgres',
                'target_conn_id': 'warehouse_bigquery',
            },
        )
        
        extract_customers = PythonOperator(
            task_id='extract_customers',
            python_callable=extract_customers_incremental,
            op_kwargs={
                'source_conn_id': 'source_postgres',
                'target_conn_id': 'warehouse_bigquery',
            },
        )
        
        extract_products = PythonOperator(
            task_id='extract_products',
            python_callable=extract_products_full,
            op_kwargs={
                'source_conn_id': 'source_postgres',
                'target_conn_id': 'warehouse_bigquery',
            },
        )
    
    # =========================================
    # TRANSFORMATION (dbt)
    # =========================================
    with TaskGroup(group_id='transformation') as transformation_group:
        
        dbt_staging = BashOperator(
            task_id='dbt_run_staging',
            bash_command='cd /opt/dbt && dbt run --select staging',
            env={
                'DBT_TARGET': 'prod',
                'DBT_PROFILES_DIR': '/opt/dbt',
            },
        )
        
        dbt_intermediate = BashOperator(
            task_id='dbt_run_intermediate',
            bash_command='cd /opt/dbt && dbt run --select intermediate',
        )
        
        dbt_marts = BashOperator(
            task_id='dbt_run_marts',
            bash_command='cd /opt/dbt && dbt run --select marts',
        )
        
        dbt_staging >> dbt_intermediate >> dbt_marts
    
    # =========================================
    # DATA QUALITY
    # =========================================
    with TaskGroup(group_id='quality_checks') as quality_group:
        
        dbt_test = BashOperator(
            task_id='dbt_test',
            bash_command='cd /opt/dbt && dbt test',
        )
        
        great_expectations = PythonOperator(
            task_id='run_great_expectations',
            python_callable=run_ge_checkpoint,
            op_kwargs={'checkpoint_name': 'daily_validation'},
        )
        
        freshness_check = PythonOperator(
            task_id='check_data_freshness',
            python_callable=check_freshness,
            op_kwargs={
                'tables': ['fct_orders', 'dim_customers'],
                'max_delay_hours': 2,
            },
        )
    
    # =========================================
    # NOTIFICATIONS
    # =========================================
    notify_success = SlackWebhookOperator(
        task_id='notify_success',
        http_conn_id='slack_webhook',
        message=":white_check_mark: Daily ETL completed successfully",
        channel='#data-alerts',
        trigger_rule='all_success',
    )
    
    notify_failure = SlackWebhookOperator(
        task_id='notify_failure',
        http_conn_id='slack_webhook',
        message=":x: Daily ETL failed!  Check Airflow logs.",
        channel='#data-alerts',
        trigger_rule='one_failed',
    )
    
    # =========================================
    # TASK DEPENDENCIES
    # =========================================
    extraction_group >> transformation_group >> quality_group
    quality_group >> [notify_success, notify_failure]
```

### Backfill DAG

```python
# dags/backfill_dag. py

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow. models import Variable

def run_backfill(start_date: str, end_date: str, table: str, **context):
    """
    Idempotent backfill function. 
    
    Partitions the date range and processes each partition independently.
    """
    from dateutil.parser import parse
    from dateutil. rrule import rrule, DAILY
    
    start = parse(start_date)
    end = parse(end_date)
    
    # Process each day independently for idempotency
    for dt in rrule(DAILY, dtstart=start, until=end):
        partition_date = dt. strftime('%Y-%m-%d')
        
        # Delete existing partition data (idempotent)
        delete_partition(table, partition_date)
        
        # Reload partition
        load_partition(table, partition_date)
        
        # Validate partition
        validate_partition(table, partition_date)
        
        context['ti'].xcom_push(
            key=f'processed_{partition_date}',
            value={'status': 'success', 'rows': get_partition_count(table, partition_date)}
        )

with DAG(
    dag_id='data_backfill',
    default_args=default_args,
    schedule_interval=None,  # Manual trigger only
    start_date=datetime(2024, 1, 1),
    catchup=False,
    params={
        'start_date': '2024-01-01',
        'end_date': '2024-01-31',
        'table': 'fct_orders',
    },
    tags=['backfill', 'manual'],
) as dag:
    
    backfill_task = PythonOperator(
        task_id='run_backfill',
        python_callable=run_backfill,
        op_kwargs={
            'start_date': '{{ params.start_date }}',
            'end_date': '{{ params.end_date }}',
            'table': '{{ params.table }}',
        },
    )
```

---

## ðŸ“Š DATA QUALITY

### Great Expectations

```python
# data_quality/great_expectations/expectations/orders_expectations.py

import great_expectations as gx

def create_orders_expectations(context: gx.DataContext):
    """Define expectations for orders data."""
    
    # Get data source
    datasource = context.get_datasource("warehouse")
    
    # Create expectation suite
    suite = context.add_or_update_expectation_suite("orders_suite")
    
    # Get batch for validation
    batch_request = datasource.get_batch_request(
        data_asset_name="fct_orders",
    )
    
    validator = context.get_validator(
        batch_request=batch_request,
        expectation_suite_name="orders_suite",
    )
    
    # =========================================
    # COMPLETENESS
    # =========================================
    validator.expect_column_to_exist("order_id")
    validator.expect_column_to_exist("customer_key")
    validator. expect_column_to_exist("total_cents")
    
    validator.expect_column_values_to_not_be_null("order_id")
    validator.expect_column_values_to_not_be_null("customer_key")
    validator.expect_column_values_to_not_be_null("total_cents")
    validator.expect_column_values_to_not_be_null("order_status")
    
    # =========================================
    # VALIDITY
    # =========================================
    validator. expect_column_values_to_be_unique("order_id")
    
    validator.expect_column_values_to_be_in_set(
        "order_status",
        ["pending", "processing", "shipped", "delivered", "cancelled", "refunded"]
    )
    
    validator.expect_column_values_to_be_between(
        "total_cents",
        min_value=0,
        max_value=100000000  # $1M max
    )
    
    validator.expect_column_values_to_match_regex(
        "order_id",
        regex=r"^ORD-\d{10}$"
    )
    
    # =========================================
    # CONSISTENCY
    # =========================================
    # Total should equal sum of components
    validator.expect_column_pair_values_A_to_be_greater_than_B(
        "total_cents",
        "discount_cents",
        or_equal=True
    )
    
    # =========================================
    # TIMELINESS
    # =========================================
    validator.expect_column_values_to_be_dateutil_parseable("ordered_at")
    
    validator.expect_column_max_to_be_between(
        "ordered_at",
        min_value=datetime. now() - timedelta(days=1),
        max_value=datetime.now() + timedelta(hours=1)
    )
    
    # =========================================
    # VOLUME
    # =========================================
    validator. expect_table_row_count_to_be_between(
        min_value=1000,  # Minimum expected daily orders
        max_value=1000000
    )
    
    # Save suite
    validator.save_expectation_suite(discard_failed_expectations=False)
    
    return suite
```

### Data Quality Monitoring

```python
# quality/monitors. py

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Optional
import logging

logger = logging.getLogger(__name__)

@dataclass
class QualityMetric:
    """Data quality metric result."""
    metric_name: str
    table_name: str
    column_name: Optional[str]
    value: float
    threshold: float
    passed: bool
    checked_at: datetime
    
@dataclass
class QualityReport:
    """Aggregated quality report."""
    table_name: str
    total_checks: int
    passed_checks: int
    failed_checks: int
    metrics: list[QualityMetric]
    overall_score: float  # 0-100

class DataQualityMonitor:
    """
    Monitors data quality metrics and alerts on issues.
    """
    
    def __init__(self, connection, alert_threshold: float = 0. 95):
        self. connection = connection
        self.alert_threshold = alert_threshold
    
    def check_completeness(
        self,
        table: str,
        column: str,
        threshold: float = 0.99
    ) -> QualityMetric:
        """Check for null/missing values."""
        query = f"""
            SELECT 
                COUNT(*) as total,
                COUNT({column}) as non_null
            FROM {table}
        """
        result = self.connection. execute(query).fetchone()
        completeness = result['non_null'] / result['total'] if result['total'] > 0 else 0
        
        return QualityMetric(
            metric_name='completeness',
            table_name=table,
            column_name=column,
            value=completeness,
            threshold=threshold,
            passed=completeness >= threshold,
            checked_at=datetime.now()
        )
    
    def check_uniqueness(
        self,
        table: str,
        column: str,
        threshold: float = 1.0
    ) -> QualityMetric:
        """Check for duplicate values."""
        query = f"""
            SELECT 
                COUNT(*) as total,
                COUNT(DISTINCT {column}) as distinct_count
            FROM {table}
        """
        result = self. connection.execute(query).fetchone()
        uniqueness = result['distinct_count'] / result['total'] if result['total'] > 0 else 0
        
        return QualityMetric(
            metric_name='uniqueness',
            table_name=table,
            column_name=column,
            value=uniqueness,
            threshold=threshold,
            passed=uniqueness >= threshold,
            checked_at=datetime.now()
        )
    
    def check_freshness(
        self,
        table: str,
        timestamp_column: str,
        max_delay_hours: int = 24
    ) -> QualityMetric:
        """Check data freshness."""
        query = f"""
            SELECT MAX({timestamp_column}) as latest
            FROM {table}
        """
        result = self.connection.execute(query). fetchone()
        latest = result['latest']
        
        if latest is None:
            delay_hours = float('inf')
        else:
            delay_hours = (datetime.now() - latest).total_seconds() / 3600
        
        return QualityMetric(
            metric_name='freshness',
            table_name=table,
            column_name=timestamp_column,
            value=delay_hours,
            threshold=max_delay_hours,
            passed=delay_hours <= max_delay_hours,
            checked_at=datetime.now()
        )
    
    def check_volume_anomaly(
        self,
        table: str,
        date_column: str,
        lookback_days: int = 30,
        std_threshold: float = 3.0
    ) -> QualityMetric:
        """Detect volume anomalies using statistical methods."""
        query = f"""
            WITH daily_counts AS (
                SELECT 
                    DATE({date_column}) as dt,
                    COUNT(*) as row_count
                FROM {table}
                WHERE {date_column} >= CURRENT_DATE - INTERVAL '{lookback_days} days'
                GROUP BY 1
            ),
            stats AS (
                SELECT 
                    AVG(row_count) as mean,
                    STDDEV(row_count) as std
                FROM daily_counts
                WHERE dt < CURRENT_DATE
            ),
            today AS (
                SELECT row_count
                FROM daily_counts
                WHERE dt = CURRENT_DATE
            )
            SELECT 
                today.row_count,
                stats.mean,
                stats. std,
                ABS(today.row_count - stats.mean) / NULLIF(stats.std, 0) as z_score
            FROM today, stats
        """
        result = self. connection.execute(query).fetchone()
        z_score = result['z_score'] or 0
        
        return QualityMetric(
            metric_name='volume_anomaly',
            table_name=table,
            column_name=None,
            value=z_score,
            threshold=std_threshold,
            passed=z_score <= std_threshold,
            checked_at=datetime.now()
        )
    
    def run_full_check(self, table: str, config: dict) -> QualityReport:
        """Run all quality checks for a table."""
        metrics = []
        
        # Completeness checks
        for column in config. get('required_columns', []):
            metrics.append(self. check_completeness(table, column))
        
        # Uniqueness checks
        for column in config. get('unique_columns', []):
            metrics.append(self.check_uniqueness(table, column))
        
        # Freshness check
        if 'timestamp_column' in config:
            metrics.append(self.check_freshness(
                table,
                config['timestamp_column'],
                config. get('max_delay_hours', 24)
            ))
        
        # Volume check
        if 'date_column' in config:
            metrics.append(self. check_volume_anomaly(table, config['date_column']))
        
        passed = sum(1 for m in metrics if m.passed)
        
        return QualityReport(
            table_name=table,
            total_checks=len(metrics),
            passed_checks=passed,
            failed_checks=len(metrics) - passed,
            metrics=metrics,
            overall_score=(passed / len(metrics) * 100) if metrics else 100
        )
```

---

## ðŸ” DATA SECURITY & COMPLIANCE

### PII Handling

```python
# security/pii_handler.py

import hashlib
import re
from typing import Optional
from cryptography.fernet import Fernet
from enum import Enum

class PIIType(Enum):
    EMAIL = "email"
    PHONE = "phone"
    SSN = "ssn"
    CREDIT_CARD = "credit_card"
    NAME = "name"
    ADDRESS = "address"

class PIIHandler:
    """
    Handles PII detection, masking, and encryption.
    """
    
    # Regex patterns for PII detection
    PATTERNS = {
        PIIType.EMAIL: r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        PIIType.PHONE: r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        PIIType. SSN: r'\b\d{3}[-]?\d{2}[-]?\d{4}\b',
        PIIType. CREDIT_CARD: r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
    }
    
    def __init__(self, encryption_key: Optional[bytes] = None):
        self. encryption_key = encryption_key
        self.cipher = Fernet(encryption_key) if encryption_key else None
    
    def detect_pii(self, text: str) -> dict[PIIType, list[str]]:
        """Detect PII in text."""
        found = {}
        for pii_type, pattern in self.PATTERNS.items():
            matches = re.findall(pattern, text)
            if matches:
                found[pii_type] = matches
        return found
    
    def mask(self, value: str, pii_type: PIIType) -> str:
        """Mask PII value."""
        if pii_type == PIIType.EMAIL:
            local, domain = value.split('@')
            return f"{local[0]}***@{domain}"
        
        elif pii_type == PIIType.PHONE:
            digits = re.sub(r'\D', '', value)
            return f"***-***-{digits[-4:]}"
        
        elif pii_type == PIIType.SSN:
            return "***-**-" + value[-4:]
        
        elif pii_type == PIIType. CREDIT_CARD:
            digits = re.sub(r'\D', '', value)
            return f"****-****-****-{digits[-4:]}"
        
        elif pii_type == PIIType.NAME:
            parts = value.split()
            return " ". join([p[0] + "***" for p in parts])
        
        else:
            return "***"
    
    def hash(self, value: str, salt: str = "") -> str:
        """One-way hash for pseudonymization."""
        return hashlib.sha256((value + salt).encode()).hexdigest()
    
    def encrypt(self, value: str) -> bytes:
        """Reversible encryption."""
        if not self.cipher:
            raise ValueError("Encryption key not configured")
        return self.cipher.encrypt(value.encode())
    
    def decrypt(self, encrypted_value: bytes) -> str:
        """Decrypt encrypted value."""
        if not self.cipher:
            raise ValueError("Encryption key not configured")
        return self.cipher.decrypt(encrypted_value).decode()


# SQL transformation for PII masking
PII_MASKING_SQL = """
CREATE OR REPLACE VIEW vw_customers_masked AS
SELECT
    customer_id,
    
    -- Mask email: show first char and domain
    CONCAT(
        LEFT(email, 1),
        '***@',
        SPLIT_PART(email, '@', 2)
    ) AS email_masked,
    
    -- Mask name: show initials only
    CONCAT(
        LEFT(first_name, 1),
        '***'
    ) AS first_name_masked,
    CONCAT(
        LEFT(last_name, 1),
        '***'
    ) AS last_name_masked,
    
    -- Hash for analytics (consistent pseudonymization)
    SHA256(CONCAT(email, '{{ var("pii_salt") }}')) AS email_hash,
    
    -- Non-PII fields pass through
    customer_segment,
    created_at,
    lifetime_value_tier
    
FROM {{ ref('dim_customers') }}
WHERE is_current = TRUE;
"""
```

### Access Control

```sql
-- Row-level security for multi-tenant data

-- Create security policy
CREATE POLICY tenant_isolation ON fct_orders
    FOR ALL
    USING (tenant_id = current_setting('app.tenant_id')::INT);

-- Enable RLS
ALTER TABLE fct_orders ENABLE ROW LEVEL SECURITY;

-- Grant access
GRANT SELECT ON fct_orders TO analyst_role;
GRANT SELECT, INSERT, UPDATE ON fct_orders TO etl_role;

-- Column-level security (views)
CREATE VIEW analyst_orders AS
SELECT
    order_id,
    order_date,
    -- Exclude sensitive columns
    -- customer_email,
    -- payment_details,
    total_cents,
    order_status
FROM fct_orders;

GRANT SELECT ON analyst_orders TO analyst_role;
```

---

## ðŸ“ˆ OBSERVABILITY & MONITORING

### Pipeline Metrics

```python
# monitoring/metrics.py

from prometheus_client import Counter, Histogram, Gauge, Info
import time
from functools import wraps

# Define metrics
PIPELINE_RUNS = Counter(
    'pipeline_runs_total',
    'Total pipeline runs',
    ['pipeline_name', 'status']
)

PIPELINE_DURATION = Histogram(
    'pipeline_duration_seconds',
    'Pipeline execution duration',
    ['pipeline_name'],
    buckets=[60, 300, 600, 1800, 3600, 7200]
)

ROWS_PROCESSED = Counter(
    'rows_processed_total',
    'Total rows processed',
    ['pipeline_name', 'table_name', 'operation']
)

DATA_FRESHNESS = Gauge(
    'data_freshness_seconds',
    'Seconds since last data update',
    ['table_name']
)

DATA_QUALITY_SCORE = Gauge(
    'data_quality_score',
    'Data quality score (0-100)',
    ['table_name']
)

PIPELINE_INFO = Info(
    'pipeline',
    'Pipeline information',
    ['pipeline_name']
)

def track_pipeline(pipeline_name: str):
    """Decorator to track pipeline execution."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time. time()
            status = 'success'
            
            try:
                result = func(*args, **kwargs)
                
                # Track rows if result contains count
                if hasattr(result, 'rows_processed'):
                    ROWS_PROCESSED. labels(
                        pipeline_name=pipeline_name,
                        table_name=result.table_name,
                        operation='processed'
                    ). inc(result.rows_processed)
                
                return result
                
            except Exception as e:
                status = 'failure'
                raise
                
            finally:
                duration = time. time() - start_time
                PIPELINE_RUNS.labels(
                    pipeline_name=pipeline_name,
                    status=status
                ).inc()
                PIPELINE_DURATION. labels(
                    pipeline_name=pipeline_name
                ).observe(duration)
        
        return wrapper
    return decorator

# Usage
@track_pipeline('daily_orders_etl')
def run_orders_pipeline():
    # Pipeline logic
    pass
```

### Alerting Configuration

```yaml
# monitoring/alerts/data_alerts.yml

groups:
  - name: data_pipeline_alerts
    rules:
      - alert: PipelineFailure
        expr: increase(pipeline_runs_total{status="failure"}[1h]) > 0
        for: 0m
        labels:
          severity: critical
          team: data-engineering
        annotations:
          summary: "Pipeline {{ $labels.pipeline_name }} failed"
          description: "Pipeline has failed in the last hour.  Check Airflow logs."
          runbook_url: "https://runbooks. company.com/pipeline-failure"

      - alert: DataFreshnessAlert
        expr: data_freshness_seconds > 7200  # 2 hours
        for: 15m
        labels:
          severity: warning
          team: data-engineering
        annotations:
          summary: "Data freshness SLA breach for {{ $labels.table_name }}"
          description: "Table {{ $labels.table_name }} has not been updated in {{ $value | humanizeDuration }}"

      - alert: DataQualityDegradation
        expr: data_quality_score < 95
        for: 30m
        labels:
          severity: warning
          team: data-engineering
        annotations:
          summary: "Data quality score dropped for {{ $labels.table_name }}"
          description: "Quality score is {{ $value }}%, below 95% threshold"

      - alert: PipelineDurationAnomaly
        expr: |
          pipeline_duration_seconds > 
          (avg_over_time(pipeline_duration_seconds[7d]) * 2)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Pipeline {{ $labels.pipeline_name }} running longer than usual"
          description: "Current duration is 2x the 7-day average"

      - alert: HighRowCountAnomaly
        expr: |
          abs(
            increase(rows_processed_total[1d]) - 
            avg_over_time(increase(rows_processed_total[1d])[7d:1d])
          ) > (stddev_over_time(increase(rows_processed_total[1d])[7d:1d]) * 3)
        for: 0m
        labels:
          severity: info
        annotations:
          summary: "Unusual row count for {{ $labels.table_name }}"
          description: "Row count deviates more than 3 standard deviations from average"
```

---

## ðŸ§ª DATA ENGINEERING VERIFICATION PLAN

```markdown
## ðŸ§ª Data Pipeline Verification

### Pipeline Testing Checklist

#### Unit Tests
- [ ] Individual transformation functions
- [ ] Schema validation logic
- [ ] Data quality rule implementations
- [ ] Error handling paths

#### Integration Tests
- [ ] Source connectivity
- [ ] Full pipeline execution (dev data)
- [ ] Incremental logic correctness
- [ ] Idempotency verification

#### Data Quality Tests
- [ ] Completeness: No unexpected nulls
- [ ] Uniqueness: No duplicate keys
- [ ] Validity: Values within expected ranges
- [ ] Consistency: Cross-table relationships valid
- [ ] Timeliness: Data freshness within SLA

### Pre-Production Checklist
- [ ] Schema changes reviewed
- [ ] Backward compatibility verified
- [ ] Performance benchmarked
- [ ] Rollback plan documented
- [ ] Monitoring alerts configured
- [ ] Data lineage updated
- [ ] Documentation updated

### Deployment Verification
- [ ] Pipeline runs successfully
- [ ] Row counts within expected range
- [ ] Quality scores above threshold
- [ ] Downstream dependencies notified
- [ ] Stakeholder sign-off
```

---

## ðŸ“ RESPONSE TEMPLATE (Data Engineer)

```markdown
## ðŸ” Analysis
[Data requirements, source analysis, volume estimates]

## ðŸ”„ Reuse Audit
[Existing pipelines, shared utilities, established patterns]

## ðŸ“ Design
[Data model, pipeline architecture, orchestration approach]

## ðŸ’» Implementation
[Pipeline code, transformations, quality checks]

## ðŸ§ª Verification Plan
[Test strategy, quality gates, monitoring setup]

## ðŸ“¦ Delivery Summary
[Tables affected, dependencies, deployment steps]
```

---

## ðŸš« DATA ENGINEERING ANTI-PATTERNS

| Avoid | Why | Instead |
|-------|-----|---------|
| Non-idempotent pipelines | Can't safely retry | Design for re-runnability |
| Modifying raw data | Loses source of truth | Write to new tables/layers |
| Unbounded incremental | Memory issues | Partition and limit batches |
| Hardcoded dates/values | Brittle, breaks | Use variables and configs |
| No data quality checks | Silent failures | Validate at every stage |
| Monolithic pipelines | Hard to debug/maintain | Modular, single-purpose steps |
| SELECT * | Schema changes break things | Explicit column selection |
| No monitoring | Undetected failures | Metrics, alerts, lineage |
| PII in logs | Compliance violation | Mask/redact sensitive data |
| No documentation | Knowledge silos | Document as you build |
| Ignoring late-arriving data | Data loss | Lookback windows |
| No backfill strategy | Can't recover history | Design for backfill from start |